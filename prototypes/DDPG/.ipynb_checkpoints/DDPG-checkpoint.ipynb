{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d596bd1aa18e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;31m# rewards_DQN_dueling = learn_episodic_DQN(N_EPS, 500, use_dueling=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m \u001b[0mrewards_DDPG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn_episodic_DDPG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPS\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoving_average\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards_DDPG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"DDPG\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-d596bd1aa18e>\u001b[0m in \u001b[0;36mlearn_episodic_DDPG\u001b[0;34m(N_eps)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;31m#             writer.add_scalar(\"action\", action, T)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;31m# print(action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0mnext_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m             \u001b[0mtotal_r\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gym/lib/python3.7/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/gym/lib/python3.7/site-packages/gym/envs/classic_control/pendulum.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, u)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mcosts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mangle_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m.1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mthdot\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m.001\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mnewthdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthdot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mnewth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnewthdot\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mnewthdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewthdot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_speed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_speed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#pylint: disable=E1111\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.distributions import Normal\n",
    "\n",
    "def moving_average(x, N):\n",
    "    return np.convolve(x, np.ones(N, ), mode='valid') / N\n",
    "\n",
    "# taken from openAI baselines\n",
    "class LinearSchedule(object):\n",
    "    def __init__(self, schedule_timesteps, final_p, initial_p=1.0):\n",
    "        self.schedule_timesteps = schedule_timesteps\n",
    "        self.final_p = final_p\n",
    "        self.initial_p = initial_p\n",
    "\n",
    "    def value(self, t):\n",
    "        \"\"\"See Schedule.value\"\"\"\n",
    "        fraction = min(float(t) / self.schedule_timesteps, 1.0)\n",
    "        return self.initial_p + fraction * (self.final_p - self.initial_p)\n",
    "\n",
    "N_EPS = 500\n",
    "\n",
    "class Policy_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy_net, self).__init__()\n",
    "        self.affine1 = nn.Linear(3, 200)\n",
    "        self.affine2 = nn.Linear(200, 100)\n",
    "        self.mean_head = nn.Linear(100, 1)\n",
    "        # self.sigma = torch.nn.Parameter(torch.tensor([self.sigma], requires_grad=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.affine1(x))\n",
    "        x = F.relu(self.affine2(x))\n",
    "        # -2 to 2 with tanh\n",
    "        mean = 2*torch.tanh(self.mean_head(x))\n",
    "        return  mean\n",
    "\n",
    "class Q_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Q_net, self).__init__()\n",
    "        action_space = 1\n",
    "        self.affine1 = nn.Linear(3 + action_space, 200)\n",
    "        self.affine2 = nn.Linear(200, 100)\n",
    "        self.value_head = nn.Linear(100, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.affine1(x))\n",
    "        x = F.relu(self.affine2(x))\n",
    "        return self.value_head(x)\n",
    "    \n",
    "Transition = namedtuple('Transition', ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# https://github.com/ikostrikov/pytorch-ddpg-naf/blob/master/ddpg.py#L11\n",
    "def soft_update(target, source, tau):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "\n",
    "# https://github.com/ikostrikov/pytorch-ddpg-naf/blob/master/ddpg.py#L15\n",
    "def hard_update(target, source):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(param.data)\n",
    "\n",
    "Q_value_net = Q_net()\n",
    "Q_value_net_target = Q_net()\n",
    "hard_update(Q_value_net_target, Q_value_net)\n",
    "\n",
    "policy_net = Policy_net()\n",
    "policy_net_target = Policy_net()\n",
    "hard_update(policy_net_target, policy_net)\n",
    "\n",
    "Q_value_net_target.eval()\n",
    "policy_net_target.eval()\n",
    "\n",
    "optim_q = optim.Adam(Q_value_net.parameters(), lr=1e-3)\n",
    "optim_p = optim.Adam(policy_net.parameters(), lr=1e-3)\n",
    "\n",
    "# def select_greedy(obs):\n",
    "#     with torch.no_grad():\n",
    "#         obs_ = torch.from_numpy(obs).float()\n",
    "#         values = Q_network(obs_)\n",
    "#         return torch.argmax(values.detach()).view(1, -1)\n",
    "    \n",
    "# def get_action(observation):\n",
    "#     x = torch.from_numpy(observation).float()\n",
    "#     normal = policy_net(x)\n",
    "#     return normal.sample()\n",
    "\n",
    "def get_q_inputs(state, action):\n",
    "    return torch.cat([state, action], 1)\n",
    "\n",
    "def train_on_batch(memory, batch_size, df, T):\n",
    "    # TODO-in future: remove the casting to tensors all the time\n",
    "    # Vectorized implementation\n",
    "    batch = memory.sample(batch_size)\n",
    "    # connect all batch Transitions to one tuple\n",
    "    batch_n = Transition(*zip(*batch))\n",
    "    # reshape actions so ve can collect the DQN(S_t, a_t) easily with gather\n",
    "    actions = torch.tensor(batch_n.action).float().view(-1, 1)\n",
    "    # get batch states\n",
    "#     print(batch_n.state)\n",
    "    states = torch.cat(batch_n.state).float()\n",
    "    next_states = torch.cat(batch_n.next_state).float()\n",
    "    batch_rewards = torch.cat(batch_n.reward).float().view(-1, 1)\n",
    "    \n",
    "    dones = torch.tensor(batch_n.done).float().view(-1, 1)\n",
    "    # collect only needed Q-values with corresponding actions for loss computation\n",
    "    inputs = Q_value_net(get_q_inputs(states, actions))\n",
    "    targets = batch_rewards\n",
    "    # targets += df*Q_network_target(next_states).max(1)[0].detach()*(1 - dones)\n",
    "    targets += (1-dones)*df*Q_value_net_target(torch.cat([next_states, policy_net_target(next_states).view(-1, 1)], 1))\n",
    "    \n",
    "    # critic loss\n",
    "    optim_q.zero_grad()\n",
    "    loss = F.mse_loss(inputs, targets.view(inputs.shape))\n",
    "#     writer.add_scalar(\"Critic loss\", loss.item(), T)\n",
    "    loss.backward()\n",
    "    optim_q.step()\n",
    "\n",
    "    # actor loss\n",
    "    optim_p.zero_grad()\n",
    "    loss_actor = -1 * (Q_value_net(torch.cat([states, policy_net(states).view(-1, 1)], 1))).mean()\n",
    "#     writer.add_scalar(\"Actor loss\", loss_actor.item(), T)\n",
    "    loss_actor.backward()\n",
    "    optim_q.step()\n",
    "\n",
    "    soft_update(Q_value_net_target, Q_value_net, tau=0.001)\n",
    "    soft_update(policy_net_target, policy_net, tau=0.001)\n",
    "    \n",
    "def learn_episodic_DDPG(N_eps=500): \n",
    "    \n",
    "    memory_len = 1000000\n",
    "    df = 0.99\n",
    "    batch_size = 64\n",
    "    train_freq = 16\n",
    "    T = 0\n",
    "    # target_update_freq = 1000\n",
    "    warmup_steps = 10000\n",
    "    # scheduler\n",
    "    e_s = 1.5\n",
    "    e_e = 0.10\n",
    "    N_decay = 80000\n",
    "    scheduler = LinearSchedule(N_decay, e_e, e_s)\n",
    "    \n",
    "    # replay mem\n",
    "    memory = ReplayMemory(memory_len)\n",
    "    rewards = []\n",
    "#     writer = SummaryWriter()\n",
    "\n",
    "    env = gym.make('Pendulum-v0')\n",
    "    # n_actions = env.action_space.n\n",
    "    actions = []\n",
    "    for i_episode in range(N_eps):\n",
    "        \n",
    "        observation = env.reset()\n",
    "        total_r = 0\n",
    "\n",
    "        for t in range(200):\n",
    "            T += 1\n",
    "            if T < warmup_steps:\n",
    "                action = env.action_space.sample()[0]\n",
    "                # print(action)\n",
    "            else:\n",
    "                curr_epsilon = scheduler.value(T - warmup_steps)\n",
    "                noise = np.random.normal(0, curr_epsilon)\n",
    "                action_mean = policy_net(torch.from_numpy(observation).float())\n",
    "                action = np.clip(action_mean.item() + noise , -2.0, 2.0)\n",
    "#             writer.add_scalar(\"action\", action, T)\n",
    "            # print(action)\n",
    "            next_observation, reward, done, info = env.step([action])\n",
    "            total_r += reward\n",
    "            reward = torch.tensor([reward])\n",
    "            \n",
    "            memory.push(torch.from_numpy(observation).view(1, -1), \\\n",
    "                action, reward, torch.from_numpy(next_observation).view(1, -1), float(done))\n",
    "            \n",
    "            # train the DQN\n",
    "            if T % train_freq == 0:\n",
    "                train_on_batch(memory, min(batch_size, T), df, T)\n",
    "            \n",
    "            observation = next_observation\n",
    "                \n",
    "            if done:\n",
    "                print(done)\n",
    "#                 writer.add_scalar(\"Episode_reward\", total_r, i_episode)\n",
    "                if (i_episode + 1) % 100 == 0:\n",
    "                    # print('curr eps', curr_epsilon)\n",
    "                    print(\"Episode {} finished with {} total rewards, T: {}\".format(i_episode, total_r, T))\n",
    "                \n",
    "        rewards.append(total_r)\n",
    "\n",
    "    # render environment\n",
    "    for i in range(5):\n",
    "        observation = env.reset()\n",
    "        for j in range(500):\n",
    "            action_mean = policy_net(torch.from_numpy(observation).float())\n",
    "            action = np.clip(action_mean.item(), -2.0, 2.0)\n",
    "            next_observation, reward, done, info = env.step([action])\n",
    "            env.render()\n",
    "    env.close()\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "# rewards_DQN_dueling = learn_episodic_DQN(N_EPS, 500, use_dueling=True)\n",
    "rewards_DDPG = learn_episodic_DDPG(N_EPS*2)\n",
    "\n",
    "plt.plot(moving_average(rewards_DDPG, 100), label=\"DDPG\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
